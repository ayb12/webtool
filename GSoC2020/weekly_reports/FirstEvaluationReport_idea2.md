# GSoC 2020 First Evaluation Report 
### Project: New Frame-Based Image and Video Annotation Pipeline for the FrameNet Brasil Web Annotation Tool

**Organization: FrameNet Brasil (UFJF)**  
Student: Prishita Ray  
Mentors: Tiago Timponi Torrent, Ely Edison Matos, Marcelo Viridiano, Fred Belcavello  

**Link to project repository**: https://github.com/FrameNetBrasil/charon  
*Please follow the installation steps provided in the README.md file to run the application on your computer*

## Project Summary

The main motivation for this project is to create a more automated and simplified video annotation pipeline using both image and textual data for the FrameNet webtool. The existing version relies on manual annotation which is a highly tedious task. In order to annotate multimodal corpora, individual frames or ideas depicted within the video need to be extracted using corresponding audio transcriptions and identified objects. This is important to obtain fine grained semantic representations of events and entities. Moreover, the video may be presented in multiple languages that need to be detected and translated to Portuguese. Also individual objects within a video need to be tracked and identified to generate corresponding textual frame elements that can speed up the annotation process.  

## Tools and Technologies Used

The following tools and technologies were utilized to create the video preprocessing tool (Charon): 

**React-Native**- The video preprocessing tool (Charon) has been created using React Native where the user interacts with the client

**Axios**- Requests are sent or received by the client through axios to or from PHP server files.   

**Xampp (Apache Server and MySQL database)**- The PHP server files are run on the Apache server of Xampp, that provides cross-platform support. Updates to  the webtool MySQL database was done using Phpmyadmin.  

**PHP-FFMPeg**- To convert video to audio and change video file formats, the PHP-FFMPeg library was used.   

**getID3**- This library was used to obtain video file metadata for manipulation.   

**Google-Tesseract**- To extract subtitles from the video, a PHP wrapper for the Google Tesseract engine provided by thiagoalessio/Tesseract-OCR was utilized.  

**IBM Watson Speech-to-Text API**- For generating audio transcripts from the video files, I used the IBM Watson Speech-to-Text API that was invoked using the Guzzle Http package library.  

## List of deliverables vs Schedule

### Part 1: Data Import/Export Pipeline

While all the user have to do is import a video file, what happens under the hood is mildly more complex:

#### The workflow

1. User import a video file (via direct upload or importing from a url);
2. File/URL is validated (checking that the URL points to a video file);
3. Check the database for duplicates (inform user/drop duplicate file);
4. Check video width/height to make sure they meet a minimum constraint;
5. Non-duplicate video is uploaded/scraped and stored;
6. Audio track extracted and converted (FLAC / 44,100 Hz / Mono) and stored;
7. Video converted (MP4/H.264) and stored;
8. Video thumbnails generated and stored;
9. Audio file uploaded to Cloud Storage/Speech API;
10. Transcription returns from Cloud Speech API and stored;
11. Subtitles extracted from video with Python-tesseract and stored;
12. Transcription and subtitles synced/aligned and merged into a single text file;
13. User review the video and sentences (side by side) for validation;
14. Transcriptions and subtitles are organized into sentences and stored according to the Webtool standard;
15. Reviewed file is uploaded to the FrameNet Webtool.

#### Timeline

The tasks specified in the workflow will be completed as follows:  
*June 1st-June 7th*: Tasks 1 to 5  
*June 8th-June 15th*: Tasks 6, 9 and 10 to generate the audio transcriptions  
*June 15th-June 22nd*: Tasks 7, 8 and 11 to generate video subtitles  
*June 22nd-July 1st*: Tasks 12 to 15, for validation, and integration into the webtool   
*July 2nd-July 3rd*: Phase 1 evaluation

## Pipeline Architecture/ Process Flow Diagram 

To describe the functionality of the preprocessing tool, the following flowchart iterates over the next steps and error conditions that are handled while interacting with the application:  

![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/GSoC2020/weekly_reports/GSoCFlowchart.jpg)

## Demonstration of Working Application

The application has three sections: the Options section allows you to upload a video file or provide a Youtube URL to download a video file for annotation, the Review section displays the video alongside the aligned and timestamped sentences generated by the preprocessing tool, and the Advanced section provides features to view the progress and customize constraints.  

This video demonstrates how the application works based on the pipeline describe above:  

![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/GSoC2020/weekly_reports/download.gif)

## Implementation 

The below section describes each step of the preprocessing pipeline in more detail:  

#### Importing from URL

After a video URL has been entered by the user, it must be sent to the pipeline which processes it through several components that are executed sequentially.

Each pipeline component is a Python class that implements a simple method. They receive an item and perform an action over it (also deciding if the item should continue through the pipeline or be dropped and no longer processed).

Typical actions include:

* validating data (checking that the URL points to a video file)
* checking for duplicates (and dropping them)
* storing the video in the database

The pipeline needs a few extra functions for processing video files:

* check video width/height to make sure they meet a minimum constraint (720x480/WideSD)
* convert all downloaded videos to a common format (MP4) and codec (H.264)
* thumbnail generation
* keep an internal queue of those media URLs which are currently being scheduled for download, and connect those responses that arrive containing the same media to that queue (this avoids downloading the same media more than once)


#### Filtering out small videos

When using the video pipeline, users might try to upload videos which are too small. The tool should restrict videos which do not have the minimum allowed size in the VIDEO_MIN_HEIGHT and VIDEO_MIN_WIDTH settings.

For example:

    VIDEO_MIN_HEIGHT = 480
    VIDEO_MIN_WIDTH = 720

It should be possible to set just one size constraint or both. When setting both of them, only videos that satisfy both minimum sizes will be saved. For the above example, videos of sizes (640 x 480) or (800 x 460) will all be dropped because at least one dimension is shorter than the constraint.

By default, there are no size constraints, so all videos are processed.


#### File storage system

The video files should be stored using a SHA1 hash of their URLs for the file names.

For example, the following video URL:

    https://youtu.be/j1IfooX0_Hw

Whose SHA1 hash is:

    abcf9fa8e0d025ed1a35e425122a4de86980334b

Will be stored in the following file:

    <VIDEOS_STORE>/full/abcf9fa8e0d025ed1a35e425122a4de86980334b.mp4

Where:

`<VIDEOS_STORE>` is the directory defined in `VIDEO_STORE` setting for the video pipeline.

`/full` is a sub-directory to separate full videos from thumbnails (if used) and video segments.

The existing database for videos will be managed using the MySQL workbench, using the DocumentMM table, that contains the path for the videos, and the SentenceMM table, that holds the sentences and associated timestamps in the video. 


#### Thumbnail generation for videos

The video pipeline should automatically create thumbnails of the downloaded videos.

In order to use this feature, users will set `IMAGES_THUMBS` to a dictionary where the keys are the thumbnail names and the values are their dimensions.

For example:

    IMAGES_THUMBS = {
        'small': (240, 180),
        'big': (320, 240),
    }

When you use this feature, the video pipeline will create thumbnails of the each specified size with this format:

    <IMAGES_STORE>/thumbs/<size_name>/<video_id>.jpg

Where:

`<size_name>` is the one specified in the `IMAGES_THUMBS` dictionary keys (small/big)
`<video_id>` is the SHA1 hash of the image url

Example of image files stored using small and big thumbnail names:

    <IMAGES_STORE>/thumbs/small/abcf9fa8e0d025ed1a35e425122a4de86980334b.jpg
    <IMAGES_STORE>/thumbs/big/abcf9fa8e0d025ed1a35e425122a4de86980334b.jpg
    
The thumbnail images will be stored in the JPEG format.

    
#### Audio Transcription

Transcriptions should have timestamps that identify the exact point in an audio/video where the given text was spoken.

    For example: 00:00:08,204 –> 00:00:10,143 - Good morning.

Timestamps will use the format `[HH:MM:SSS]` where `HH`, `MM`, `SSS` are hours, minutes and milliseconds from the beginning and ending of each sentence in the audio track.

Once finished processing, the Speech-to-Text API will return the transcription to be stored in the following file:

    <TEXT_STORE>/transcripts/<video_id>.txt

###### /* [Via IBM Watson Speech-to-text?](https://www.ibm.com/in-en/cloud/watson-speech-to-text)

#### Subtitle Extraction

*Python-tesseract is an optical character recognition (OCR) tool for python. That is, it will recognize and “read” the text embedded in images.*

*Python-tesseract is a wrapper for Google’s Tesseract-OCR Engine. It is also useful as a stand-alone invocation script to tesseract, as it can read all image types supported by the Pillow and Leptonica imaging libraries, including jpeg, png, gif, bmp, tiff, and others.*

[https://pypi.org/project/pytesseract/](https://pypi.org/project/pytesseract/)

Once finished processing, the tool will return the subtitles to be stored in the following file:

    <TEXT_STORE>/subtitles/<video_id>.srt
    
In case visual subtitles are not present in a video, only the Speech-to-Text API will be used to generate the audio transcriptions, in Portuguese.


#### Transcription-Subtitle Alignment

The tool should have an interface to compare video and text files (audio transcripts and extracted subtitles) side by side, allowing users to review, search and make the necessary edits and corrections of any errors.

This "validation interface" should have a simple UI, consisting mainly of a video viewer with playback controls (such as "play" and "stop") where users will be able to verify the temporally aligning of video and transcription (similar to YouTube's auto-generated subtitles interface).

After reviewing, the tool should merge both audio transcripts and extracted subtitles (aligned and synced according to their timestamps) into one single file, then output to a combined folder.

    <TEXT_STORE>/combined/<video_id>.json
    
The video will not be segmented, however a provision must be made to locate the start and end timestamps of a sentence that is to be annotated. 


#### Item Exporter

Once we have all of the above, we want to export those items to the webtool. For this purpose, the pipeline should provide different output formats, such as XML, CSV or JSON. The JSON format will be used for the second part of the project, i.e. object extraction.


## Weekly Reports
These are the weekly reports of my work during the first coding period from June 1- June 30 2020:

### Week 01 - (Jun 01 - Jun 07)

#### Tasks scheduled for this week (Tasks 1-5)  
1. User import a video file (via direct upload or importing from a url)
*  layout for the webtool video uploader UI [[go to Figma]](https://www.figma.com/files/project/9936175/Webtool-Video-Uploaded) ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
2. File/URL is validated (checking that the URL points to a video file) ![uat-failed](https://img.shields.io/static/v1?label=UAT&message=failed&color=red) ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow)
3. Check the database for duplicates (inform user/drop duplicate file) ![uat-failed](https://img.shields.io/static/v1?label=UAT&message=failed&color=red) ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow)
4. Check video width/height to make sure they meet a minimum constraint ![uat-passed](https://img.shields.io/static/v1?label=UAT&message=passed&color=success)
5. Non-duplicate video is uploaded/scraped and stored ![uat-failed](https://img.shields.io/static/v1?label=UAT&message=failed&color=red) ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow)

#### Challenges and solutions

I was facing some problems with implementing the size constraint for the videos. To get the dimensions of the video, I am using the getID3 php file, but that does not support all video formats (e.g. .mkv files are not supported). Moreover, it requires the actual path of the file to be uploaded on the user's computer that can lead to privacy issues. Though it works fine on my computer, this access may not be allowed on a random user's computer. So I need help to find a way to perform this check only after the file is uploaded on the server. Both getID3 and ffmpeg require the user file's path details.  

#### Tasks postponed to next week
2. File/URL is validated (checking that the URL points to a video file)
3. Check the database for duplicates (inform user/drop duplicate file) 
5. Non-duplicate video is uploaded/scraped and stored 

#### Observations
In the preprocessing tool, the user can choose to upload a video from his local computer or provide a url. In the first case, various constraints such as duplicacy of files, size constraints as well as availability of files are checked before being uploaded. If the constraints are met, the video file is stored with its SHA1 hash value on the server. In the second case, the validity of the URL is checked based on regexes for Youtube videos. The options of selecting language, corpus and document from the webtool MySQL database (webtool_db) is provided to the user as well. 

### Week 02 - (Jun 08 - Jun 14)  

#### Tasks scheduled for this week (Tasks 6, 9 and 10) 
2. File/URL is validated (checking that the URL points to a video file) ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow)
3. Check the database for duplicates (inform user/drop duplicate file) ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow)
5. Non-duplicate video is uploaded/scraped and stored ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow)
6. Audio track extracted and converted (FLAC / 44,100 Hz / Mono) and stored  ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow)
9. Audio file uploaded to Cloud Storage/Speech API ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) 
10. Transcription returns from Cloud Speech API and stored ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow)

#### Challenges and Solutions

#### Tasks postponed to next week

#### Observations
I am facing some issues with integrating the backend functions with the React frontend, especially with the modal box rendering (since the formats are different in PHP and React) and MySQL connectivity. I was working on the week 2 tasks, but then had to shift in between to React and change the platform again, therefore, this is causing some delay, but these will be resolved by tomorrow. I can then start working on the other tasks that are scheduled for this week. The React frontend files for the new tool are updated in the Github master branch. 

### Week 03 - (Jun 15 - Jun 22)  

#### Tasks scheduled for this week (Tasks 7, 8 and 11) 
2. File/URL is validated (checking that the URL points to a video file) ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
3. Check the database for duplicates (inform user/drop duplicate file) ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
5. Non-duplicate video is uploaded/scraped and stored ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green) 
6. Audio track extracted and converted (FLAC / 44,100 Hz / Mono) and stored ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)  
7.Video converted (MP4/H.264) and stored ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)  
8.Video thumbnails generated and stored ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)  
9. Audio file uploaded to Cloud Storage/Speech API ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)  
10. Transcription returns from Cloud Speech API and stored ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)  
11.Subtitles extracted from video with Python-tesseract and stored ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow)

#### Challenges and Solutions

#### Tasks postponed to next week

#### Observations

### Week 04 - (Jun 22 - Jul 1)  

#### Tasks scheduled for this week (Tasks 12, 13, 14 and 15) 
11.Subtitles extracted from video with Python-tesseract and stored ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)  
12. Transcription and subtitles synced/aligned and merged into a single text file; ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)    
13. User review the video and sentences (side by side) for validation;  ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)   
14. Transcriptions and subtitles are organized into sentences and stored according to the Webtool standard;  ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)    
15. Reviewed file is uploaded to the FrameNet Webtool.  ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)  

#### Challenges and Solutions

#### Tasks postponed to next week

#### Observations

Remember to use tags! You can add multiple tags to any task.

![completed](https://img.shields.io/static/v1?label=&message=completed&color=green) = done and ready for User Acceptance Testing (UAT)<br>
![uat-passed](https://img.shields.io/static/v1?label=UAT&message=passed&color=success) = tested and ready to merge with Master<br>
![deployed](https://img.shields.io/static/v1?label=&message=deployed&color=success) = merged with Master<br>
![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) = task deferred from one week to the next<br>
![help](https://img.shields.io/static/v1?label=&message=need_help&color=blue) = needs help from mentors<br>
![definition](https://img.shields.io/static/v1?label=&message=needs_definition&color=orange) = **blocked** task that needs discussion with mentors<br>
![important](https://img.shields.io/static/v1?label=&message=important&color=red) = something that needs to be addressed immediately<br>

Use [Shields.io](https://shields.io) to create new tags if needed.

## Self Assessment 

