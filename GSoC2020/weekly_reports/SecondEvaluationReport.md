# GSoC 2020 First Evaluation Report 
### Project: New Frame-Based Image and Video Annotation Pipeline for the FrameNet Brasil Web Annotation Tool

**Organization: FrameNet Brasil (UFJF)**  
Student: Prishita Ray  
Mentors: Tiago Timponi Torrent, Ely Edison Matos, Marcelo Viridiano, Fred Belcavello  

**Link to project repository**: https://github.com/FrameNetBrasil/charon  
*Please follow the installation steps provided in the README.md file to run the application on your computer*

## Project Summary

The main motivation for this project is to create a more automated and simplified video annotation pipeline using both image and textual data for the FrameNet webtool. The existing version relies on manual annotation which is a highly tedious task. In order to annotate multimodal corpora, individual frames or ideas depicted within the video need to be extracted using corresponding audio transcriptions and identified objects. This is important to obtain fine grained semantic representations of events and entities. Moreover, the video may be presented in multiple languages that need to be detected and translated to Portuguese. Also individual objects within a video need to be tracked and identified to generate corresponding textual frame elements that can speed up the annotation process.  

## Tools and Technologies Used

The following tools and technologies were utilized to create the video preprocessing tool (Charon): 

**React-Native**- The video preprocessing tool (Charon) has been created using React Native where the user interacts with the client

**Axios**- Requests are sent or received by the client through axios to or from PHP server files.   

**Xampp (Apache Server and MySQL database)**- The PHP server files are run on the Apache server of Xampp, that provides cross-platform support. Updates to  the webtool MySQL database was done using Phpmyadmin.  

**PHP-FFMPeg**- To convert video to audio and change video file formats, the PHP-FFMPeg library was used.   

**getID3**- This library was used to obtain video file metadata for manipulation.   

**Google-Tesseract**- To extract subtitles from the video, a PHP wrapper for the Google Tesseract engine provided by thiagoalessio/Tesseract-OCR was utilized.  

**IBM Watson Speech-to-Text API**- For generating audio transcripts from the video files, I used the IBM Watson Speech-to-Text API that was invoked using the Guzzle Http package library.  

## List of deliverables vs Schedule

### Part 2:



#### The workflow



#### Timeline

The tasks specified in the workflow will be completed as follows:  
*June 1st-June 7th*: Tasks 1 to 5  
*June 8th-June 15th*: Tasks 6, 9 and 10 to generate the audio transcriptions  
*June 15th-June 22nd*: Tasks 7, 8 and 11 to generate video subtitles  
*June 22nd-July 1st*: Tasks 12 to 15, for validation, and integration into the webtool   
*July 2nd-July 3rd*: Phase 1 evaluation

## Pipeline Architecture/ Process Flow Diagram 

To describe the functionality of the preprocessing tool, the following flowchart iterates over the next steps and error conditions that are handled while interacting with the application:  

![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/GSoC2020/weekly_reports/GSoCFlowchart.jpg)

## Demonstration of Working Application

The application has three sections: the Options section allows you to upload a video file or provide a Youtube URL to download a video file for annotation, the Review section displays the video alongside the aligned and timestamped sentences generated by the preprocessing tool, and the Advanced section provides features to view the progress and customize constraints.  

This video demonstrates how the application works based on the pipeline describe above:  

![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/GSoC2020/weekly_reports/download1.gif)
![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/GSoC2020/weekly_reports/download2.gif)
![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/GSoC2020/weekly_reports/download3.gif)
![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/GSoC2020/weekly_reports/download4.gif)

## Implementation 

The below section describes each step of the preprocessing pipeline in more detail:  



## Week 01 - (July 03 - July 10)

### Tasks scheduled for this week

1. The preprocessed video from the previous pipeline is imported into the preprocessing tool from the server. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
2. To annotate a sentence, the start and end timestamps of that sentence are chosen. ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow)
3. Run the video with each frame having a time gap of 1 second, using VATIC.js. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
4. Objects in a frame will be detected automatically using a trained object detection model, which will also create bounding boxes around them. Changes to the code for obtaining the actual pixel coordinates of bounding boxes will also be made. ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow)
5. The coordinates of the pixels that serve as corners to a detected object's bounding box will be saved in a list or csv file ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)


### Challenges and solutions

The model training is taking some time due to the complexity of the network and dataset size, which I tried to perform on my local machine, and am now running on a Kaggle kernel. The results will be ready for testing within the next few days. 


### Tasks completed

....

### Tasks postponed to next week

2. To annotate a sentence, the start and end timestamps of that sentence are chosen. 
4. Objects in a frame will be detected automatically using a trained object detection model, which will also create bounding boxes around them. Changes to the code for obtaining the actual pixel coordinates of bounding boxes will also be made.
...

### Observations

...

## Week 02 - (July 11 - July 20)

### Tasks scheduled for this week

2. To annotate a sentence, the start and end timestamps of that sentence are chosen. ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
4. Objects in a frame will be detected automatically using a trained object detection model, which will also create bounding boxes around them. Changes to the code for obtaining the actual pixel coordinates of bounding boxes will also be made. ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
6. For the following 5 frames, the KLT (Kanade-Lucas-Tomasi) feature tracking algorithm will track these objects by interpolating the current coordinates of the detected objects. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
7. The 5 frame constraint is kept for each detected object that is to be tracked, to ensure that it is present in the video for at least five seconds, otherwise tracking it is not useful and won't help in annotation. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
10. Using a windowing technique, the same object detection and tracking process from steps 4 to 9 will be followed for the duration of the video. Every new object that is tracked successfully will be added to the list storing the coordinates. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)

### Challenges and solutions


### Tasks completed

....

### Tasks postponed to next week

...

### Observations

...

## Week 03 - (July 21 - July 27)

### Tasks scheduled for this week

9. The generated images will be stored in the OBJECTS_STORE folder. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
11. The generated images will be shown to the user for validation. An option for manual creation of bounding boxes will be provided if the user is not satisfied. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
12. Identified objects in the video will be stored in the ObjectMM table of the webtool database ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)

### Challenges and solutions


---
Remember to use tags! You can add multiple tags to any task.

![completed](https://img.shields.io/static/v1?label=&message=completed&color=green) = done and ready for User Acceptance Testing (UAT)<br>
![uat-passed](https://img.shields.io/static/v1?label=UAT&message=passed&color=success) = tested and ready to merge with Master<br>
![deployed](https://img.shields.io/static/v1?label=&message=deployed&color=success) = merged with Master<br>
![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) = task deferred from one week to the next<br>
![help](https://img.shields.io/static/v1?label=&message=need_help&color=blue) = needs help from mentors<br>
![definition](https://img.shields.io/static/v1?label=&message=needs_definition&color=orange) = **blocked** task that needs discussion with mentors<br>
![important](https://img.shields.io/static/v1?label=&message=important&color=red) = something that needs to be addressed immediately<br>

Use [Shields.io](https://shields.io) to creat new tags if needed.

## Self Assessment

