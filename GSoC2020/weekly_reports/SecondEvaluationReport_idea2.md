# GSoC 2020 Second Evaluation Report 
### Project: New Frame-Based Image and Video Annotation Pipeline for the FrameNet Brasil Web Annotation Tool

**Organization: FrameNet Brasil (UFJF)**  
Student: Prishita Ray  
Mentors: Tiago Timponi Torrent, Ely Edison Matos, Marcelo Viridiano, Fred Belcavello  

**Link to project repository**: https://github.com/FrameNetBrasil/charon  
**Link to model training files**: https://github.com/FrameNetBrasil/charon_model
*Please follow the installation steps provided in the README.md file to run the application on your computer*

## Project Summary

The main motivation for this project is to create a more automated and simplified video annotation pipeline using both image and textual data for the FrameNet webtool. The existing version relies on manual annotation which is a highly tedious task. In order to annotate multimodal corpora, individual frames or ideas depicted within the video need to be extracted using corresponding audio transcriptions and identified objects. This is important to obtain fine grained semantic representations of events and entities. Moreover, the video may be presented in multiple languages that need to be detected and translated to Portuguese. Also individual objects within a video need to be tracked and identified to generate corresponding textual frame elements that can speed up the annotation process.  

## Tools and Technologies Used

The following tools and technologies were utilized to create the video preprocessing tool (Charon): 

**React-Native**- The video preprocessing tool (Charon) has been created using React Native where the user interacts with the client

**Axios**- Requests are sent or received by the client through axios to or from PHP server files.   

**Xampp (Apache Server and MySQL database)**- The PHP server files are run on the Apache server of Xampp, that provides cross-platform support. Updates to  the webtool MySQL database was done using Phpmyadmin.  

**PHP-FFMPeg**- To convert video to audio and change video file formats, the PHP-FFMPeg library was used.   

**getID3**- This library was used to obtain video file metadata for manipulation.   

**Google-Tesseract**- To extract subtitles from the video, a PHP wrapper for the Google Tesseract engine provided by thiagoalessio/Tesseract-OCR was utilized.  

**IBM Watson Speech-to-Text API**- For generating audio transcripts from the video files, I used the IBM Watson Speech-to-Text API that was invoked using the Guzzle Http package library.  

**Docker**- Docker Desktop was used to create the Linux container to run the entire application, including both the frontend web app and the backend PHP and flask servers.

**Keras**- Library was used for training the object detection model using Tensorflow 2.2 backend  

**Scikit-Image**- The functions provided within this library were used to create the object tracker from the bounding boxes generated by the trained object detection model in a video frame  

**Flask**- To create a server to run the object tracking python script on a sentence that is selected for annotation from the video.  

Other python libraries that were installed for this project include- numpy, matplotlib, opencv, scipy, mysql-connector-python, Pillow, pickle-mixin, glob3, opencv_contrib_python, mpld3 and moviepy

## List of deliverables vs Schedule

### Part 2: Semi-Automatization of the Annotation Process

The objects within a video need to be detected and tracked over time to form the image data that will be used for automated annotation  

#### The workflow

1. The preprocessed video from the previous pipeline is imported into the webtool from the server.
2. To annotate a sentence, the start and end timestamps of that sentence are chosen.
3. Run the video with each frame having a time gap of 1 second, using VATIC.js.
4. Objects in a frame will be detected automatically using YOLO (You Look Only Once), which will also create bounding boxes around them. In case COCO does not perform well, a new model will be trained using the Open Images dataset, and in addition, changes to the code for obtaining the actual pixel coordinates will also be made.
5. The coordinates of the pixels that serve as corners to a detected object's bounding box will be saved in a list.
6. For the following 5 frames, the KLT (Kanade-Lucas-Tomasi) feature tracking algorithm will track these objects by interpolating the current coordinates of the detected objects.
7. The 5 frame constraint is kept for each detected object that is to be tracked, to ensure that it is present in the video for at least five seconds, otherwise tracking it is not useful and won't help in annotation.
8. If the image for an object is generated after the previous step is performed, a minimum size constraint and image quality resolution will have to be met to save the image.
9. The generated images will be stored in the OBJECTS_STORE folder.
10. Using a windowing technique, the same object detection and tracking process from steps 4 to 9 will be followed for the duration of the video. Every new object that is tracked successfully will be added to the list storing the coordinates.
11. The generated images will be shown to the user for validation. An option for manual creation of bounding boxes will be provided if the user is not satisfied.
12. Identified objects in the video will be stored in the ObjectMM table of the webtool database


#### Timeline

The tasks specified in the workflow will be completed as follows:  
*July 3rd- July 10th*: Tasks 1 to 5 for object detection using YOLO   
*July 11th- July 20th*: Tasks 6,7 and 10 for object tracking  
*July 21st- July 25th*: Tasks 9,11 and 12 for validation and storage in file system  
*July 26th- July 27th*: Phase 2 evaluation   

## Pipeline Architecture/ Process Flow Diagram 

To describe the functionality of the preprocessing tool, the following flowchart iterates over the next steps and error conditions that are handled while interacting with the application:  

![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/GSoC2020/weekly_reports/GSoCFlowchart.jpg)

## Demonstration of Working Application

The application has three sections: the Options section allows you to upload a video file or provide a Youtube URL to download a video file for annotation, the Review section displays the video alongside the aligned and timestamped sentences generated by the preprocessing tool, and the Advanced section provides features to view the progress and customize constraints.  

This video demonstrates how the application works based on the pipeline describe above:  

![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/GSoC2020/weekly_reports/download1.gif)
![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/GSoC2020/weekly_reports/download2.gif)
![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/GSoC2020/weekly_reports/download3.gif)
![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/GSoC2020/weekly_reports/download4.gif)

## Implementation 

The below section describes each step of the object tracking pipeline in more detail:  

#### Running videos with VATIC.js

The preprocessed video imported into the webtool will be run in frames of fixed duration (i.e. 1 second) by adjusting the speed multiplier option in VATIC. This step is important to effectively detect objects in the given frame, with as much accuracy as possible, and minimizing noise due to movements.  


#### Object Detection

In each frame, the YOLO (You Look Only Once) model using neural networks which is trained on an image database such as ImageNet, will help in identifying objects of interest. The DarkNet framework can be installed as mentioned here: https://medium.com/analytics-vidhya/installing-darknet-on-windows-462d84840e5a. Since VATIC is a javascript video annotation tool, to perform these image processing tasks, the OpenCV javascript module will be used. However, a drawback of OpenCV is that it directly gives the centre coordinates x and y probabilities, width(w) and height(h) from the actual pixel coordinates, without returning the original coordinates i.e. x_start, x_end, y_start, y_end of the detected objects. 

![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/YOLO.JPG) 

    x = (x_mean- x_start)/x_start
    y = (y_mean- y_start)/y_start
    w = actual_width/frame_width
    h = actual_height/frame_height 
    
In order to get these original pixel coordinates, a modification to the file that generates these values can be made as follows:

    x_start = round(x- w/2)
    y_start = round(y- h/2)
    x_end = x_start + w
    y_end = y_start + h
    
This gives back the original pixel coordinate values for the object.


#### Object Tracking

After the objects have been detected in the first frame, they will be tracked for the next five frames to ensure that they are present in the video, and are significant to the annotation process. For this, the Kanade Lucas Tomasi (KLT) feature tracking algorithm will be used: https://www.learnopencv.com/object-tracking-using-opencv-cpp-python/ 

 Based on the bounding boxes of the objects, the algorithm will use them as interest points for local optimization. KLT uses a squared distance  criterion to check for transformation parameters such as displacement in x and y  from the original positions. Thus, the bounded boxes of the identified objects are  expanded using interpolation while they are being tracked with the help of these  transformation parameters as features. KLT is chosen as it is able to handle occlusions very well, and can track major transformations over shorter periods of  time efficiently too.

Two cases that may arise are as follows:

* An object detected in the first frame does not persist in the video for the next 5 frames. In that case, the coordinates of the object will be discarded and it will not be considered for annotation.
* The object is present in the video for the next 5 frames. Its original coordinates are then retained and an image is generated from the bounding boxes.

In the second case, an image will be generated for the object based on its bounding box coordinates.

This same procedure is followed using a windowing technique for every set of five frames for the duration of the video, to identify all possible objects. 


#### User Validation

Based on the objects identified by the model, the user has to make a decision to accept or reject them. Therefore, an option is provided to the user for manual bounding box creation in case he is not satisfied. 


#### Saving Identified Objects in File Storage System

Before the images of the tracked objects can be stored in the file system, they need to meet certain size and quality requirements, to ensure better accuracy for annotation. Moreover, duplicate images of the same object need to be discarded.

For example:

    OBJECT_MIN_HEIGHT =  800 pixels
    OBJECT_MIN_WIDTH = 800 pixels
    OBJECT_QUALITY = 200 DPI (Dots per inch)

A folder to store the images of these objects will be created as follows:

    <OBJECTS_STORE>/<video_id>/image_name.JPG
    
The image name will be updated sequentially for every new object being stored.

## Week 01 - (July 03 - July 10)

### Tasks scheduled for this week

1. The preprocessed video from the previous pipeline is imported into the preprocessing tool from the server. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
2. To annotate a sentence, the start and end timestamps of that sentence are chosen. ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow)
3. Run the video with each frame having a time gap of 1 second, using VATIC.js. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
4. Objects in a frame will be detected automatically using a trained object detection model, which will also create bounding boxes around them. Changes to the code for obtaining the actual pixel coordinates of bounding boxes will also be made. ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow)
5. The coordinates of the pixels that serve as corners to a detected object's bounding box will be saved in a list or csv file ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)


### Challenges and solutions

The model training is taking some time due to the complexity of the network and dataset size, which I tried to perform on my local machine, and am now running on a Kaggle kernel. The results will be ready for testing within the next few days. 


### Tasks completed

....

### Tasks postponed to next week

2. To annotate a sentence, the start and end timestamps of that sentence are chosen. 
4. Objects in a frame will be detected automatically using a trained object detection model, which will also create bounding boxes around them. Changes to the code for obtaining the actual pixel coordinates of bounding boxes will also be made.
...

### Observations

...

## Week 02 - (July 11 - July 20)

### Tasks scheduled for this week

2. To annotate a sentence, the start and end timestamps of that sentence are chosen. ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
4. Objects in a frame will be detected automatically using a trained object detection model, which will also create bounding boxes around them. Changes to the code for obtaining the actual pixel coordinates of bounding boxes will also be made. ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
6. For the following 5 frames, the KLT (Kanade-Lucas-Tomasi) feature tracking algorithm will track these objects by interpolating the current coordinates of the detected objects. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
7. The 5 frame constraint is kept for each detected object that is to be tracked, to ensure that it is present in the video for at least five seconds, otherwise tracking it is not useful and won't help in annotation. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
10. Using a windowing technique, the same object detection and tracking process from steps 4 to 9 will be followed for the duration of the video. Every new object that is tracked successfully will be added to the list storing the coordinates. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)

### Challenges and solutions


### Tasks completed

....

### Tasks postponed to next week

...

### Observations

...

## Week 03 - (July 21 - July 27)

### Tasks scheduled for this week

9. The generated images will be stored in the OBJECTS_STORE folder. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
11. The generated images will be shown to the user for validation. An option for manual creation of bounding boxes will be provided if the user is not satisfied. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
12. Identified objects in the video will be stored in the ObjectMM table of the webtool database ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)

### Challenges and solutions


---
Remember to use tags! You can add multiple tags to any task.

![completed](https://img.shields.io/static/v1?label=&message=completed&color=green) = done and ready for User Acceptance Testing (UAT)<br>
![uat-passed](https://img.shields.io/static/v1?label=UAT&message=passed&color=success) = tested and ready to merge with Master<br>
![deployed](https://img.shields.io/static/v1?label=&message=deployed&color=success) = merged with Master<br>
![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) = task deferred from one week to the next<br>
![help](https://img.shields.io/static/v1?label=&message=need_help&color=blue) = needs help from mentors<br>
![definition](https://img.shields.io/static/v1?label=&message=needs_definition&color=orange) = **blocked** task that needs discussion with mentors<br>
![important](https://img.shields.io/static/v1?label=&message=important&color=red) = something that needs to be addressed immediately<br>

Use [Shields.io](https://shields.io) to creat new tags if needed.

## Self Assessment

