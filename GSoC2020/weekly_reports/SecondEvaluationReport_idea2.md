# GSoC 2020 Second Evaluation Report 
### Project: New Frame-Based Image and Video Annotation Pipeline for the FrameNet Brasil Web Annotation Tool

**Organization: FrameNet Brasil (UFJF)**  
Student: Prishita Ray  
Mentors: Tiago Timponi Torrent, Ely Edison Matos, Marcelo Viridiano, Fred Belcavello  

**Link to project repository**: https://github.com/FrameNetBrasil/charon  
**Link to model training files**: https://github.com/FrameNetBrasil/charon_model  
*Please follow the installation steps provided in the README.md file to run the application on your computer*

## Project Summary

The main motivation for this project is to create a more automated and simplified video annotation pipeline using both image and textual data for the FrameNet webtool. The existing version relies on manual annotation which is a highly tedious task. In order to annotate multimodal corpora, individual frames or ideas depicted within the video need to be extracted using corresponding audio transcriptions and identified objects. This is important to obtain fine grained semantic representations of events and entities. Moreover, the video may be presented in multiple languages that need to be detected and translated to Portuguese. Also individual objects within a video need to be tracked and identified to generate corresponding textual frame elements that can speed up the annotation process.  

## Tools and Technologies Used

The following tools and technologies were utilized to create the video preprocessing tool (Charon): 

**React-Native**- The video preprocessing tool (Charon) has been created using React Native where the user interacts with the client

**Axios**- Requests are sent or received by the client through axios to or from PHP server files.   

**Xampp (Apache Server and MySQL database)**- The PHP server files are run on the Apache server of Xampp, that provides cross-platform support. Updates to  the webtool MySQL database was done using Phpmyadmin.  

**PHP-FFMPeg**- To convert video to audio and change video file formats, the PHP-FFMPeg library was used.   

**getID3**- This library was used to obtain video file metadata for manipulation.   

**Google-Tesseract**- To extract subtitles from the video, a PHP wrapper for the Google Tesseract engine provided by thiagoalessio/Tesseract-OCR was utilized.  

**IBM Watson Speech-to-Text API**- For generating audio transcripts from the video files, I used the IBM Watson Speech-to-Text API that was invoked using the Guzzle Http package library.  

**Docker**- Docker Desktop was used to create the Linux container to run the entire application, including both the frontend web app and the backend PHP and flask servers.

**Keras**- Library was used for training the object detection model using Tensorflow 2.2 backend  

**Scikit-Image**- The functions provided within this library were used to create the object tracker from the bounding boxes generated by the trained object detection model in a video frame  

**Flask**- To create a server to run the object tracking python script on a sentence that is selected for annotation from the video.  

Other python libraries that were installed for this project include- numpy, matplotlib, opencv, scipy, mysql-connector-python, Pillow, pickle-mixin, glob3, opencv_contrib_python, mpld3 and moviepy

## List of deliverables vs Schedule

### Part 2: Semi-Automatization of the Annotation Process

The objects within a video need to be detected and tracked over time to form the image data that will be used for automated annotation  

#### The workflow

1. The preprocessed video from the previous pipeline is imported into the webtool from the server.
2. To annotate a sentence, the start and end timestamps of that sentence are chosen.
3. Run the video with each frame having a time gap of 1 second.
4. Objects in a frame will be detected automatically using YOLO (You Look Only Once), which will also create bounding boxes around them. In case COCO does not perform well, a new model will be trained using the Open Images dataset, and in addition, changes to the code for obtaining the actual pixel coordinates will also be made.
5. The coordinates of the pixels that serve as corners to a detected object's bounding box will be saved in a list.
6. For the following 5 frames, the KLT (Kanade-Lucas-Tomasi) feature tracking algorithm will track these objects by interpolating the current coordinates of the detected objects.
7. The 5 frame constraint is kept for each detected object that is to be tracked, to ensure that it is present in the video for at least five seconds, otherwise tracking it is not useful and won't help in annotation.
8. If the image for an object is generated after the previous step is performed, a minimum size constraint and image quality resolution will have to be met to save the image.
9. The generated images will be stored in the OBJECTS_STORE folder.
10. Using a windowing technique, the same object detection and tracking process from steps 4 to 9 will be followed for the duration of the video. Every new object that is tracked successfully will be added to the list storing the coordinates.
11. The generated images will be shown to the user for validation. An option for manual creation of bounding boxes will be provided if the user is not satisfied.
12. Identified objects in the video will be stored in the ObjectMM table of the webtool database


#### Timeline

The tasks specified in the workflow will be completed as follows:  
*July 3rd- July 10th*: Tasks 1 to 5 for object detection using YOLO   
*July 11th- July 20th*: Tasks 6,7 and 10 for object tracking  
*July 21st- July 25th*: Tasks 9,11 and 12 for validation and storage in file system  
*July 26th- July 27th*: Phase 2 evaluation   

## Pipeline Architecture/ Process Flow Diagram 

To describe the functionality of the preprocessing tool, the following flowchart iterates over the next steps and error conditions that are handled while interacting with the application:  

![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/GSoC2020/weekly_reports/GSoCFlowchart2.jpg)

### Deep Learning Models 
The YOLO (You Look only Once) object detection model, based on the DarkNet architecture was trained on multiple different datasets and can predict bounding boxes and captions for identified objects within an image. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes.
It is highly accurate, takes one pass for prediction and is less time-consuming. The architecture of the YOLO model can be observed below:

![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/GSoC2020/weekly_reports/yoloarch.JPG)

RetinaNet is a single, unified network composed of a backbone network and two task-specific subnetworks. It is based on the ResNet101 architecture using Convolutional Neural Networks. This model takes a lot of time to train but is also a one-pass prediction algorithm like YOLO. The architecture of RetinaNet can be observed below:

![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/GSoC2020/weekly_reports/rnet.JPG)

### Datasets 
For this project, I have used the COCO (Common Objects in Context) dataset along with the YOLO object detection model, which has 80 classes. COCO is a large-scale object detection, segmentation, and captioning dataset, which is commonly used for object detection purposes. This dataset identifies objects that can be maooed to lexical units within the FrameNet database. 

I also tried using a custom dataset of 120 classes from the Open Images Dataset by Google, which contains a total of 600 classes in combination with the Retinanet object detection model. 

## Implementation 

The below section describes each step of the object tracking pipeline in more detail:  

#### Running videos and extracting frames from them

The sentence that is selected from the preprocessed video imported into the webtool will be used to clip the video for its duration, and this video clip will be run in equally spaced frames (i.e. of 1 second). This step is important to effectively detect objects in the given frame, with as much accuracy as possible, and minimizing noise due to movements. The python OpenCV module along with moviepy will be used for frame extraction and video processing. 


#### Object Detection

In each frame, the YOLO (You Look Only Once) model using neural networks which is trained on an image database such as COCO, will help in identifying objects of interest. The DarkNet framework for the YOLO model can be installed as mentioned here: https://medium.com/analytics-vidhya/installing-darknet-on-windows-462d84840e5a. A drawback of the OpenCV module is that it directly gives the centre coordinates x and y probabilities, width(w) and height(h) from the actual pixel coordinates, without returning the original coordinates i.e. x_start, x_end, y_start, y_end of the detected objects. 

![alt text](https://github.com/FrameNetBrasil/webtool/blob/gsoc2020_2/YOLO.JPG) 

    x = (x_mean- x_start)/x_start
    y = (y_mean- y_start)/y_start
    w = actual_width/frame_width
    h = actual_height/frame_height 
    
In order to get these original pixel coordinates, a modification to the file that generates these values can be made as follows:

    x_start = round(x- w/2)
    y_start = round(y- h/2)
    x_end = x_start + w
    y_end = y_start + h
    
This gives back the original pixel coordinate values for the object.


#### Object Tracking

After the objects have been detected in the first frame, they will be tracked for the next ten frames to ensure that they are present in the video, and are significant to the annotation process. For this, the Kanade Lucas Tomasi (KLT) feature tracking algorithm will be used: https://www.learnopencv.com/object-tracking-using-opencv-cpp-python/ 

 Based on the bounding boxes of the objects, the algorithm will use them as interest points for local optimization. KLT uses a squared distance  criterion to check for transformation parameters such as displacement in x and y from the original positions. Thus, the bounded boxes of the identified objects are  expanded using interpolation while they are being tracked with the help of these  transformation parameters as features. KLT is chosen as it is able to handle occlusions very well, and can track major transformations over shorter periods of time efficiently too.

Two cases that may arise are as follows:

* An object detected in the first frame does not persist in the video for the next 10 frames. In that case, the coordinates of the object will be discarded and it will not be considered for annotation.
* The object is present in the video for the next 10 frames. Its original coordinates are then retained and an image is generated from the bounding boxes.

In the second case, an image will be generated for the object based on its bounding box coordinates.

This same procedure is followed using a windowing technique for every set of ten frames for the duration of the video, to identify all possible objects. 


#### User Validation

Based on the objects identified by the model, the user has to make a decision to accept or reject them. Therefore, an option is provided to the user for manual bounding box creation in case he is not satisfied. 


#### Saving Identified Objects in File Storage System

Before the images of the tracked objects can be stored in the file system, they need to meet certain size and quality requirements, to ensure better accuracy for annotation. Moreover, duplicate images of the same object need to be discarded.

For example:

    OBJECT_MIN_HEIGHT =  800 pixels
    OBJECT_MIN_WIDTH = 800 pixels
    OBJECT_QUALITY = 200 DPI (Dots per inch)

A folder to store the images of these objects will be created as follows:

    <OBJECTS_STORE>frameid_objectid.PNG
    
The image name will be updated sequentially for every new object being stored.

## Week 01 - (July 03 - July 10)

### Tasks scheduled for this week

1. The preprocessed video from the previous pipeline is imported into the preprocessing tool from the server. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
2. To annotate a sentence, the start and end timestamps of that sentence are chosen. ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow)
3. Run the video with each frame having a time gap of 1 second. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
4. Objects in a frame will be detected automatically using a trained object detection model, which will also create bounding boxes around them. Changes to the code for obtaining the actual pixel coordinates of bounding boxes will also be made. ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow)
5. The coordinates of the pixels that serve as corners to a detected object's bounding box will be saved in a list or csv file ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)


### Challenges and solutions

The model training took some time due to the complexity of the network and dataset size, which I tried to perform on my local machine, and then ran on a Kaggle kernel. I was able to train the model in the following week. 


### Tasks completed
1. The preprocessed video from the previous pipeline is imported into the preprocessing tool from the server.
3. Run the video with each frame having a time gap of 1 second.  
5. The coordinates of the pixels that serve as corners to a detected object's bounding box will be saved in a list or csv file.

### Tasks postponed to next week

2. To annotate a sentence, the start and end timestamps of that sentence are chosen. 
4. Objects in a frame will be detected automatically using a trained object detection model, which will also create bounding boxes around them. Changes to the code for obtaining the actual pixel coordinates of bounding boxes will also be made.

### Observations

Given that YOLO is a popular model that is used for object detection purposes, I tried to train it on the COCO (Common Objects in Context) dataset and obtained good results. I also tried an alternative method by creating my own dataset from the Google Open Images Dataset, and tried to train it using a Retinanet model, as both YOLO and Retinanet are single pass object detectors unlike Mask-RCNNs and similar networks that are double pass object detectors, but the results were not as encouraging, and the model took a long time to train. So I have considered using the former method of YOLO trained on the COCO dataset. 

## Week 02 - (July 11 - July 20)

### Tasks scheduled for this week

2. To annotate a sentence, the start and end timestamps of that sentence are chosen. ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
4. Objects in a frame will be detected automatically using a trained object detection model, which will also create bounding boxes around them. Changes to the code for obtaining the actual pixel coordinates of bounding boxes will also be made. ![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
6. For the following 5 frames, the KLT (Kanade-Lucas-Tomasi) feature tracking algorithm will track these objects by interpolating the current coordinates of the detected objects. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
7. The 5 frame constraint is kept for each detected object that is to be tracked, to ensure that it is present in the video for at least five seconds, otherwise tracking it is not useful and won't help in annotation. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
10. Using a windowing technique, the same object detection and tracking process from steps 4 to 9 will be followed for the duration of the video. Every new object that is tracked successfully will be added to the list storing the coordinates. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)

### Challenges and solutions

It was important to uniquely identify objects belonging to the same class under different names: for example, if multiple person objects are present within the same video frame, they must be identified uniquely. For this purpose, the model had to be able to create bounding boxes in a way that each of these objects had adequate representation, among the stored pictures and minimize overlap conditions. Another important challenge was to be able to identify objects and track them even with a lot of noise within the video. Also some objects only had a part of their actual real-world shape in a frame, so that had to be identified as well. The program was modified to handle these cases properly. 


### Tasks completed

2. To annotate a sentence, the start and end timestamps of that sentence are chosen. 
4. Objects in a frame will be detected automatically using a trained object detection model, which will also create bounding boxes around them. Changes to the code for obtaining the actual pixel coordinates of bounding boxes will also be made. 
6. For the following 5 frames, the KLT (Kanade-Lucas-Tomasi) feature tracking algorithm will track these objects by interpolating the current coordinates of the detected objects. 
7. The 5 frame constraint is kept for each detected object that is to be tracked, to ensure that it is present in the video for at least five seconds, otherwise tracking it is not useful and won't help in annotation. 
10. Using a windowing technique, the same object detection and tracking process from steps 4 to 9 will be followed for the duration of the video. Every new object that is tracked successfully will be added to the list storing the coordinates. 

### Tasks postponed to next week

All tasks for this week were completed. 

### Observations

Most objects were found to be present in the entire duration of a 10 frames window, so it was not required to discard them. Moreover, as the actual frame width and height is lesser than the object image size constraints, the objects detected and tracked were of smaller size than the frame and they automatically met the constraints. The object tracking worked well after the objects were detected accurately in the first frame of a window. 

## Week 03 - (July 21 - July 27)

### Tasks scheduled for this week

9. The generated images will be stored in the OBJECTS_STORE folder. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
11. The generated images will be shown to the user for validation. An option for manual creation of bounding boxes will be provided if the user is not satisfied. ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)
12. Identified objects in the video will be stored in the ObjectMM table of the webtool database ![completed](https://img.shields.io/static/v1?label=&message=completed&color=green)

### Challenges and solutions

In case the user opted for manual creation of bounding boxes after the vaidation step, it was necessary to accept the corresponding object classes along with the drawn bounding boxes. Moreover, the number of objects could vary in a video so that had to be handled as well. Also while updating the ObjectMM table, some foreign keys related to frame element and annotation set had to be temporarily removed as they would be done in the later course of this project and to prevent referential integrity errors.  

### Tasks completed

9. The generated images will be stored in the OBJECTS_STORE folder. 
11. The generated images will be shown to the user for validation. An option for manual creation of bounding boxes will be provided if the user is not satisfied. 
12. Identified objects in the video will be stored in the ObjectMM table of the webtool database 

### Tasks postponed to next week

All tasks for this week were completed.

### Observations

After the objects are tracked successfully, they are being stored as pictures in the Object_Store folder, and an entry is made for each object for their ID and corresponding class in the object_annotations.csv file. 

---
Remember to use tags! You can add multiple tags to any task.

![completed](https://img.shields.io/static/v1?label=&message=completed&color=green) = done and ready for User Acceptance Testing (UAT)<br>
![uat-passed](https://img.shields.io/static/v1?label=UAT&message=passed&color=success) = tested and ready to merge with Master<br>
![deployed](https://img.shields.io/static/v1?label=&message=deployed&color=success) = merged with Master<br>
![carryover](https://img.shields.io/static/v1?label=&message=carryover&color=yellow) = task deferred from one week to the next<br>
![help](https://img.shields.io/static/v1?label=&message=need_help&color=blue) = needs help from mentors<br>
![definition](https://img.shields.io/static/v1?label=&message=needs_definition&color=orange) = **blocked** task that needs discussion with mentors<br>
![important](https://img.shields.io/static/v1?label=&message=important&color=red) = something that needs to be addressed immediately<br>

Use [Shields.io](https://shields.io) to create new tags if needed.

## Self Assessment

In this month, I worked on integrating an object tracking module with the video annotation tool that I created last month. Before starting with the training, I researched about object detection models that would be most suitable for this project, in terms of training time, accuracy and speed of detection. There are many object detection datasets available such as COCO, ImageNet, Open Images etc. but it was necessary to choose one that could detect objects that were related to the FrameNet lexical units in some way. Thereafter, I studied about the architecture of these models, the number of layers in them, functions of each layer and their training mechanisms. I finally narrowed down to two architectures: The YOLO model and RetinaNet. Due to limited computational resources, I had to train the models on the cloud. I initially tried to use Google Colab, and then moved on to Kaggle kernels due to their 12-hour restriction on training time. So it took some time to ensure that I was using the best possible approach with my available resources to perform this task with a good accuracy, and it did serve good results later on. This helped me realize that it is more important to plan well and follow a structured approach before starting with the actual work at any point of time. I was in contact with my mentors the entire time and they helped solve any doubts that I had during the course of the work. They also provided valuable suggestions whenever I was stuck and that really helped me to continue with the work. I was also able to figure out some errors in the code after they pointed them out. There were some improvements that I made to the previous month's work, so that it would be easier to integrate the current work with it. 
